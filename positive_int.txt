Let D= the number we wish to convert from decimal to binary
Find P, such that 2^P is the largest power of two smaller than D.
Repeat until P<0
If 2^P<=D then
put 1 into column P
subtract 2^P from D
Else
put 0 into column P
End if
Subtract 1 from P


short int length = 2^16
int length = 2^32
unsigned int length = 2^32

BIT Logic:
1001 = (2^3)+(0)+(0)+(2^0) = 9

1101 = (2^4)+....+(2^0)

etc.


When the number is too long for the type, the computer reads the bits that are within it's memory allocation and returns whatever number those bit parts represent.
